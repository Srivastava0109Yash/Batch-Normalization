{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81c5d51",
   "metadata": {},
   "source": [
    "## Q1. Theory and Concepts:\n",
    "\n",
    "### 1. Explain the concept of batch normalization in the context of Artificial Neural Networks.\n",
    "\n",
    "Batch normalization is a technique used in Artificial Neural Networks (ANNs) to improve the training and performance of deep learning models. It aims to address the problem of internal covariate shift, which refers to the change in the distribution of network activations as the parameters of the previous layers change during training. This shift can slow down the learning process and make it difficult for the network to converge.\n",
    "Batch normalization normalizes the input to each layer by subtracting the batch mean and dividing by the batch standard deviation. This normalization step ensures that the inputs to each layer have zero mean and unit variance. By doing so, it helps in reducing the internal covariate shift and makes the training process more stable.\n",
    "\n",
    "### 2. Describe the benefits of using batch normalization during training.\n",
    "\n",
    "The benefits of using batch normalization during training are as follows:\n",
    "\n",
    "- a. Improved convergence: Batch normalization reduces the internal covariate shift, which helps in stabilizing the training process. It enables the use of higher learning rates and accelerates the convergence of the network. This can lead to faster training and reduced training time.\n",
    "\n",
    "- b. Regularization effect: Batch normalization acts as a form of regularization by adding a small amount of noise to the network activations. This noise helps in reducing overfitting and improves the generalization ability of the model.\n",
    "\n",
    "- c. Reduces dependency on initialization: Batch normalization reduces the dependence of the network on the choice of initial parameter values. It allows the network to converge and perform well even with suboptimal initialization, making the training process more robust.\n",
    "\n",
    "- d. Increased network stability: Batch normalization reduces the impact of small changes in the input distribution on the network's behavior. This stability makes the network less sensitive to changes in hyperparameters or the order of training examples in a mini-batch.\n",
    "\n",
    "- e. Enables the use of deeper networks: Batch normalization helps in training deeper neural networks. It mitigates the vanishing/exploding gradient problem by keeping the activations within a reasonable range. This allows the gradients to flow more effectively through the network, enabling the training of deeper architectures.\n",
    "\n",
    "### 3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters.\n",
    "\n",
    "The working principle of batch normalization involves two main steps: normalization and learnable parameters.\n",
    "\n",
    "- a. Normalization step: In the normalization step, batch normalization normalizes the inputs to each layer by subtracting the mean and dividing by the standard deviation of the batch. For a given layer's activation values, the mean and standard deviation are computed across the mini-batch during training. This normalization step ensures that the inputs have zero mean and unit variance, which helps in stabilizing the training process.\n",
    "\n",
    "- b. Learnable parameters: Batch normalization introduces two learnable parameters per feature dimension: a scale parameter (gamma) and a shift parameter (beta). These parameters are applied after the normalization step. The scale parameter allows the network to learn the optimal scale of the normalized activations, while the shift parameter allows it to learn the optimal shift. By learning these parameters, the network can adapt the normalized activations to the specific requirements of the task.\n",
    "\n",
    "During training, the learnable parameters are updated using backpropagation and gradient descent, just like other parameters in the network. During inference or testing, the batch statistics are typically replaced with running averages computed during training to normalize the inputs using the learned parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76c7ca",
   "metadata": {},
   "source": [
    "## Q2. Impementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05516607",
   "metadata": {},
   "source": [
    "- Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.\n",
    "\n",
    "- Implement a simple feedforward neural network using any deep learning framework/library (e.g.,Tensorflow, PyTorch).\n",
    "- Train the neural network on the chosen dataset without using batch normalization.\n",
    "- Implement batch normalization layers in the neural network and train the model again.\n",
    "- Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization.\n",
    "- Discuss the impact of batch normalization on the training process and the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c92978",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  let's consider the MNIST dataset, which consists of grayscale images of handwritten digits (0-9)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c679fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dataset\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13939652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a5b84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a7f291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35054b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and reshape:\n",
    "\n",
    "x_train=x_train.reshape(-1,784)/255.0\n",
    "x_test=x_test.reshape(-1,784)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25663528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE:\n",
    "\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84082344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b688b",
   "metadata": {},
   "source": [
    "### we'll implement a simple feedforward neural network without batch normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f251eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.3417 - accuracy: 0.9045 - val_loss: 0.1652 - val_accuracy: 0.9524\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1371 - accuracy: 0.9603 - val_loss: 0.1213 - val_accuracy: 0.9632\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0959 - accuracy: 0.9714 - val_loss: 0.1088 - val_accuracy: 0.9660\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0730 - accuracy: 0.9781 - val_loss: 0.1003 - val_accuracy: 0.9682\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0577 - accuracy: 0.9820 - val_loss: 0.0762 - val_accuracy: 0.9767\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0465 - accuracy: 0.9861 - val_loss: 0.0734 - val_accuracy: 0.9762\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0387 - accuracy: 0.9883 - val_loss: 0.0736 - val_accuracy: 0.9756\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0304 - accuracy: 0.9912 - val_loss: 0.0720 - val_accuracy: 0.9787\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0248 - accuracy: 0.9926 - val_loss: 0.0710 - val_accuracy: 0.9787\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 0.0722 - val_accuracy: 0.9798\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0189 - accuracy: 0.9943 - val_loss: 0.0765 - val_accuracy: 0.9781\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0146 - accuracy: 0.9958 - val_loss: 0.0770 - val_accuracy: 0.9782\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0136 - accuracy: 0.9958 - val_loss: 0.0834 - val_accuracy: 0.9789\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.1076 - val_accuracy: 0.9702\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.0903 - val_accuracy: 0.9760\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0996 - val_accuracy: 0.9750\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0870 - val_accuracy: 0.9793\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 0.0973 - val_accuracy: 0.9778\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0863 - val_accuracy: 0.9799\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0999 - val_accuracy: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d35e4afc70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# model without batch normalization:\n",
    "\n",
    "model_no_bn = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(784,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_no_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_no_bn.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f029305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0999 - accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models on the test set\n",
    "_, acc_no_bn = model_no_bn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "566f5ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model without batch normalization - Test Accuracy: 0.9764999747276306\n"
     ]
    }
   ],
   "source": [
    "print(\"Model without batch normalization - Test Accuracy:\", acc_no_bn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535e795",
   "metadata": {},
   "source": [
    "### let's implement the same neural network architecture but with batch normalization layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e3be24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 5s 9ms/step - loss: 0.2944 - accuracy: 0.9162 - val_loss: 0.1509 - val_accuracy: 0.9543\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1433 - accuracy: 0.9596 - val_loss: 0.1188 - val_accuracy: 0.9654\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1064 - accuracy: 0.9686 - val_loss: 0.1035 - val_accuracy: 0.9685\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0856 - accuracy: 0.9749 - val_loss: 0.0937 - val_accuracy: 0.9716\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0724 - accuracy: 0.9785 - val_loss: 0.0946 - val_accuracy: 0.9725\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0624 - accuracy: 0.9815 - val_loss: 0.0871 - val_accuracy: 0.9724\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0896 - val_accuracy: 0.9719\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0487 - accuracy: 0.9848 - val_loss: 0.0889 - val_accuracy: 0.9719\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0440 - accuracy: 0.9865 - val_loss: 0.0881 - val_accuracy: 0.9739\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0376 - accuracy: 0.9886 - val_loss: 0.0992 - val_accuracy: 0.9695\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0370 - accuracy: 0.9886 - val_loss: 0.0858 - val_accuracy: 0.9731\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0333 - accuracy: 0.9897 - val_loss: 0.0927 - val_accuracy: 0.9733\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0309 - accuracy: 0.9898 - val_loss: 0.0900 - val_accuracy: 0.9725\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0305 - accuracy: 0.9900 - val_loss: 0.0982 - val_accuracy: 0.9735\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0272 - accuracy: 0.9914 - val_loss: 0.0989 - val_accuracy: 0.9736\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0968 - val_accuracy: 0.9743\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0216 - accuracy: 0.9934 - val_loss: 0.1015 - val_accuracy: 0.9724\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.0982 - val_accuracy: 0.9723\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0204 - accuracy: 0.9936 - val_loss: 0.1048 - val_accuracy: 0.9719\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.0208 - accuracy: 0.9935 - val_loss: 0.0973 - val_accuracy: 0.9740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d37ce73730>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Define the model architecture with batch normalization\n",
    "model_bn = Sequential([\n",
    "    Dense(128, input_shape=(784,)),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_bn.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2eb0c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0973 - accuracy: 0.9740\n",
      "Model with batch normalization - Test Accuracy: 0.9739999771118164\n"
     ]
    }
   ],
   "source": [
    "_, acc_bn = model_bn.evaluate(x_test, y_test)\n",
    "print(\"Model with batch normalization - Test Accuracy:\", acc_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b50c65",
   "metadata": {},
   "source": [
    "__Benefits of batch normalization__\n",
    "- Training Stability\n",
    "- Generalization\n",
    "- Faster Convergence\n",
    "- Reduced Dependency on Initialization\n",
    "- Improved Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97534c43",
   "metadata": {},
   "source": [
    "### Q3. Experimentation and analysis\n",
    "\n",
    "\n",
    "- Experiment with different batch sizes and observe the effect on the training dynamics and model performancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0a7ccb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2732 - accuracy: 0.9185 - val_loss: 0.1397 - val_accuracy: 0.9582\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1543 - accuracy: 0.9536 - val_loss: 0.1100 - val_accuracy: 0.9673\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1255 - accuracy: 0.9613 - val_loss: 0.0971 - val_accuracy: 0.9690\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1109 - accuracy: 0.9662 - val_loss: 0.0922 - val_accuracy: 0.9691\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0969 - accuracy: 0.9694 - val_loss: 0.0899 - val_accuracy: 0.9708\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0895 - accuracy: 0.9718 - val_loss: 0.0831 - val_accuracy: 0.9740\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0835 - accuracy: 0.9737 - val_loss: 0.0887 - val_accuracy: 0.9730\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0768 - accuracy: 0.9755 - val_loss: 0.0863 - val_accuracy: 0.9735\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0744 - accuracy: 0.9761 - val_loss: 0.0839 - val_accuracy: 0.9734\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0707 - accuracy: 0.9765 - val_loss: 0.0838 - val_accuracy: 0.9752\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0838 - accuracy: 0.9752\n",
      "Batch Size: 32\n",
      "Test Accuracy: 0.9751999974250793\n",
      "------------------------------------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 11s 10ms/step - loss: 0.2616 - accuracy: 0.9237 - val_loss: 0.1462 - val_accuracy: 0.9566\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.1364 - accuracy: 0.9597 - val_loss: 0.1087 - val_accuracy: 0.9674\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 9s 9ms/step - loss: 0.1051 - accuracy: 0.9682 - val_loss: 0.0991 - val_accuracy: 0.9692\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 0.0896 - accuracy: 0.9721 - val_loss: 0.1016 - val_accuracy: 0.9700\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 0.0791 - accuracy: 0.9747 - val_loss: 0.0920 - val_accuracy: 0.9733\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0709 - accuracy: 0.9774 - val_loss: 0.0949 - val_accuracy: 0.9735\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.0653 - accuracy: 0.9791 - val_loss: 0.0902 - val_accuracy: 0.9732\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 0.0596 - accuracy: 0.9810 - val_loss: 0.0950 - val_accuracy: 0.9734\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0539 - accuracy: 0.9824 - val_loss: 0.0888 - val_accuracy: 0.9731\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0497 - accuracy: 0.9837 - val_loss: 0.0934 - val_accuracy: 0.9738\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0934 - accuracy: 0.9738\n",
      "Batch Size: 64\n",
      "Test Accuracy: 0.973800003528595\n",
      "------------------------------------\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 6s 10ms/step - loss: 0.2980 - accuracy: 0.9150 - val_loss: 0.1590 - val_accuracy: 0.9539\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1441 - accuracy: 0.9587 - val_loss: 0.1208 - val_accuracy: 0.9655\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1066 - accuracy: 0.9693 - val_loss: 0.1009 - val_accuracy: 0.9698\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0869 - accuracy: 0.9743 - val_loss: 0.1026 - val_accuracy: 0.9694\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0722 - accuracy: 0.9779 - val_loss: 0.0913 - val_accuracy: 0.9721\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0646 - accuracy: 0.9803 - val_loss: 0.0881 - val_accuracy: 0.9732\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0543 - accuracy: 0.9840 - val_loss: 0.0836 - val_accuracy: 0.9747\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0496 - accuracy: 0.9853 - val_loss: 0.0880 - val_accuracy: 0.9736\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0442 - accuracy: 0.9861 - val_loss: 0.0884 - val_accuracy: 0.9749\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0394 - accuracy: 0.9878 - val_loss: 0.0877 - val_accuracy: 0.9741\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0877 - accuracy: 0.9741\n",
      "Batch Size: 128\n",
      "Test Accuracy: 0.9740999937057495\n",
      "------------------------------------\n",
      "Epoch 1/10\n",
      "235/235 [==============================] - 4s 12ms/step - loss: 0.3428 - accuracy: 0.9020 - val_loss: 0.2173 - val_accuracy: 0.9461\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 3s 11ms/step - loss: 0.1624 - accuracy: 0.9549 - val_loss: 0.1451 - val_accuracy: 0.9577\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.1187 - accuracy: 0.9668 - val_loss: 0.1191 - val_accuracy: 0.9665\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 3s 11ms/step - loss: 0.0935 - accuracy: 0.9735 - val_loss: 0.1107 - val_accuracy: 0.9665\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.0787 - accuracy: 0.9772 - val_loss: 0.0960 - val_accuracy: 0.9710\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.0658 - accuracy: 0.9813 - val_loss: 0.0954 - val_accuracy: 0.9692\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.0564 - accuracy: 0.9835 - val_loss: 0.0942 - val_accuracy: 0.9715\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 0.0489 - accuracy: 0.9858 - val_loss: 0.0942 - val_accuracy: 0.9716\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.0429 - accuracy: 0.9871 - val_loss: 0.0932 - val_accuracy: 0.9720\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.0374 - accuracy: 0.9894 - val_loss: 0.0883 - val_accuracy: 0.9714\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0883 - accuracy: 0.9714\n",
      "Batch Size: 256\n",
      "Test Accuracy: 0.9714000225067139\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Define a list of different batch sizes to experiment with\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Create a new instance of the model to ensure a fresh start for each experiment\n",
    "    model_bn = Sequential([\n",
    "        Dense(128, input_shape=(784,)),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model with the current batch size\n",
    "    model_bn.fit(x_train, y_train, batch_size=batch_size, epochs=10, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    _, test_accuracy = model_bn.evaluate(x_test, y_test)\n",
    "    \n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5c18d",
   "metadata": {},
   "source": [
    "### Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35c782",
   "metadata": {},
   "source": [
    "**Advantages of Batch Normalization:**\n",
    "\n",
    "- Improved Training Dynamics: Batch normalization helps in stabilizing the training process by reducing internal covariate shift. This can lead to faster convergence, smoother loss curves, and improved training dynamics. With proper batch normalization, the network can learn more efficiently and effectively.\n",
    "\n",
    "- Regularization Effect: Batch normalization adds noise to the network activations during training, acting as a form of regularization. This can reduce overfitting and improve generalization performance, especially when training data is limited. By reducing the reliance on individual training examples, batch normalization encourages the network to learn more robust and generalizable representations.\n",
    "\n",
    "- Higher Learning Rates: Batch normalization allows for the use of higher learning rates without compromising training stability. This accelerates the convergence of the network and reduces the training time. The ability to use larger learning rates can be particularly beneficial in deep neural networks.\n",
    "\n",
    "- Reduction in Weight Initialization Sensitivity: Batch normalization reduces the dependence on the choice of initial parameter values. It helps in mitigating issues such as vanishing or exploding gradients, making the training process more robust. Batch normalization can help deep networks converge even with suboptimal weight initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53032c15",
   "metadata": {},
   "source": [
    "__Potential Limitations of Batch Normalization:__\n",
    "\n",
    "- Batch Size Sensitivity: Batch normalization performance can vary with different batch sizes. Small batch sizes may lead to noisy estimates of the batch statistics, reducing the effectiveness of normalization. On the other hand, very large batch sizes may reduce the regularization effect and limit the network's ability to generalize. It is important to choose an appropriate batch size based on the specific dataset and network architecture.\n",
    "\n",
    "- Inference Dependency: During inference, batch normalization requires access to batch statistics (mean and variance) calculated during training. This introduces a dependency on the batch size and ordering of examples during inference. In certain scenarios, such as online or real-time prediction, this dependency may not be feasible or practical.\n",
    "\n",
    "- Computational Overhead: Batch normalization adds extra computations during both forward and backward passes, which can increase the computational overhead. This can be a concern in scenarios where efficiency is crucial, such as resource-constrained environments or when training large-scale models.\n",
    "\n",
    "- Loss of Individual Sample Information: Batch normalization focuses on the statistics of the mini-batch, which can lead to a loss of information about individual samples. In some cases, this loss of individuality may not be desirable, especially when training on small datasets with highly varied samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4da57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
